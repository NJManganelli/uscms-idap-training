{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea Processors\n",
    "Coffea relies mainly on [uproot](https://github.com/scikit-hep/uproot) to provide access to ROOT files for analysis.\n",
    "As a usual analysis will involve processing tens to thousands of files, totalling gigabytes to terabytes of data, there is a certain amount of work to be done to build a parallelized framework to process the data in a reasonable amount of time. \n",
    "\n",
    "In coffea up to 0.7 (SemVer), a `coffea.processor` module was provided to encapsulate the core functionality of the analysis, which could be run locally or distributed via a number of Execturors. This allowed users to worry just about the actual analysis code and not about how to implement efficient parallelization, assuming that the parallization is a trivial map-reduce operation (e.g. filling histograms and adding them together).\n",
    "\n",
    "In coffa 2024 (CalVer), integration with `dask` is deeper (via `dask_awkward` and `uproot.dask`), and whether an analysis is to be executed on local or distributed resources, a TaskGraph encapsulating the analysis is created. We will demonstrate how to use callable code to build these TGs \n",
    "\n",
    "(Sidenote: with some adaptations for the new version of scikit-hep/coffea, a SemVer coffea processor module's `process` function can serve as the callable function)\n",
    "\n",
    "\n",
    "Let's start by writing a simple processor class that reads some CMS open data and plots a dimuon mass spectrum.\n",
    "We'll start by copying the [ProcessorABC](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html#coffea.processor.ProcessorABC) skeleton and filling in some details:\n",
    "\n",
    " * Remove `flag`, as we won't use it\n",
    " * Adding a new histogram for $m_{\\mu \\mu}$\n",
    " * Building a [Candidate](https://coffeateam.github.io/coffea/api/coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate.html#coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate) record for muons, since we will read it with `BaseSchema` interpretation (the files used here could be read with `NanoAODSchema` but we want to show how to build vector objects from other TTree formats) \n",
    " * Calculating the dimuon invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate\n",
    "import hist\n",
    "import dask\n",
    "from hist.dask import Hist\n",
    "\n",
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        dataset = events.metadata['dataset']\n",
    "        muons = ak.zip(\n",
    "            {\n",
    "                \"pt\": events.Muon_pt,\n",
    "                \"eta\": events.Muon_eta,\n",
    "                \"phi\": events.Muon_phi,\n",
    "                \"mass\": events.Muon_mass,\n",
    "                \"charge\": events.Muon_charge,\n",
    "            },\n",
    "            with_name=\"PtEtaPhiMCandidate\",\n",
    "            behavior=candidate.behavior,\n",
    "        )\n",
    "\n",
    "        h_mass = (\n",
    "            Hist.new\n",
    "            .StrCat([\"opposite\", \"same\"], name=\"sign\")\n",
    "            .Log(1000, 0.2, 200., name=\"mass\", label=\"$m_{\\mu\\mu}$ [GeV]\")\n",
    "            .Int64()\n",
    "        )\n",
    "\n",
    "        cut = (ak.num(muons) == 2) & (ak.sum(muons.charge, axis=1) == 0)\n",
    "        # add first and second muon in every event together\n",
    "        dimuon = muons[cut][:, 0] + muons[cut][:, 1]\n",
    "        h_mass.fill(sign=\"opposite\", mass=dimuon.mass)\n",
    "\n",
    "        cut = (ak.num(muons) == 2) & (ak.sum(muons.charge, axis=1) != 0)\n",
    "        dimuon = muons[cut][:, 0] + muons[cut][:, 1]\n",
    "        h_mass.fill(sign=\"same\", mass=dimuon.mass)\n",
    "\n",
    "        return {\n",
    "            \"entries\": ak.num(events, axis=0),\n",
    "            \"mass\": h_mass,\n",
    "        }\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to just use bare uproot to execute this processor, we could do that with the following example, which:\n",
    "\n",
    " * Opens a CMS open data file\n",
    " * Creates a NanoEvents object using `BaseSchema` (roughly equivalent to the output of `uproot.lazy`)\n",
    " * Creates a `MyProcessor` instance\n",
    " * Runs the `process()` function, which returns our accumulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "\n",
    "filename = \"root://xcache//store/user/ncsmith/opendata_mirror/Run2012B_DoubleMuParked.root\"\n",
    "#file = uproot.open(filename)\n",
    "events = NanoEventsFactory.from_root(\n",
    "    {filename: \"Events\"},\n",
    "    entry_stop=10000,\n",
    "    metadata={\"dataset\": \"DoubleMuon\"},\n",
    "    schemaclass=BaseSchema,\n",
    "    delayed=True,\n",
    ").events()\n",
    "p = MyProcessor()\n",
    "taskgraph = p.process(events)\n",
    "taskgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask.visualize(taskgraph['mass'], optimize_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask.visualize(taskgraph['mass'], optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask.compute(taskgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out, *_ = dask.compute(taskgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "out[\"mass\"].plot1d(ax=ax)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend(title=\"Dimuon charge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could expand on this code to run over several chunks of the file, setting `entry_start` and `entry_stop` as appropriate. Then, several datasets could be processed by iterating over several files. However, the processor [Runner](https://coffeateam.github.io/coffea/api/coffea.processor.Runner.html) can help with this! One lists the datasets and corresponding files, the processor they want to run, and which executor they want to use. Available executors derive from `ExecutorBase` and are listed [here](https://coffeateam.github.io/coffea/modules/coffea.processor.html#classes). Since these files are very large, we limit to just reading the first few chunks of events from each dataset with `maxchunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_fileset = {\n",
    "    \"DoubleMuon\": {\n",
    "        \"files\": {\n",
    "            \"root://xcache//store/user/ncsmith/opendata_mirror/Run2012B_DoubleMuParked.root\": \"Events\",\n",
    "            \"root://xcache//store/user/ncsmith/opendata_mirror/Run2012C_DoubleMuParked.root\": \"Events\",\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"is_mc\": True,\n",
    "        },\n",
    "    },\n",
    "    \"ZZ to 4mu\": {\n",
    "        \"files\": {\n",
    "            \"root://xcache//store/user/ncsmith/opendata_mirror/ZZTo4mu.root\": \"Events\",\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"is_mc\": False,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We'll see in a separate notebook how to construct such an initial fileset using dataset discovery tools. For now, we'll take the above `initial_fileset` and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coffea.dataset_tools import apply_to_fileset, max_chunks, max_files, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_available, preprocessed_total = preprocess(\n",
    "        initial_fileset,\n",
    "        step_size=100_000,\n",
    "        align_clusters=None,\n",
    "        skip_bad_files=True,\n",
    "        recalculate_steps=False,\n",
    "        files_per_batch=1,\n",
    "        file_exceptions=(OSError,),\n",
    "        save_form=True,\n",
    "        uproot_options={},\n",
    "        step_size_safety_factor=0.5,\n",
    "    )\n",
    "    #with gzip.open(f\"{output_file}_available.json.gz\", \"wt\") as file:\n",
    "    #    print(f\"Saved available fileset chunks to {output_file}_available.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessed fileset\n",
    "Lets have a look at the contents of the preprocessed_available part of the fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterative_run = processor.Runner(\n",
    "    executor = processor.IterativeExecutor(compression=None),\n",
    "    schema=BaseSchema,\n",
    "    maxchunks=4,\n",
    ")\n",
    "\n",
    "out = iterative_run(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=MyProcessor(),\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing chunks and files\n",
    "Given this preprocessed fileset, we can test our processor on just a few chunks of a handful of files. To do this, we use the max_files and max_chunks functions from the dataset tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preprocessed_files = max_files(preprocessed_available, 1)\n",
    "test_preprocessed = max_chunks(test_preprocessed_files, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_tg, small_rep = apply_to_fileset(data_manipulation=MyProcessor(),\n",
    "                            fileset=test_preprocessed,\n",
    "                            schemaclass=BaseSchema,\n",
    "                            uproot_options={\"allow_read_errors_with_report\": (OSError, KeyError)},\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask.visualize(small_tg, optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_computed, small_rep_computed = dask.compute(small_tg, small_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_rep_computed['DoubleMuon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to use more than a single core on our machine, we simply change [IterativeExecutor](https://coffeateam.github.io/coffea/api/coffea.processor.IterativeExecutor.html) for [FuturesExecutor](https://coffeateam.github.io/coffea/api/coffea.processor.FuturesExecutor.html), which uses the python [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) standard library. We can then set the most interesting argument to the `FuturesExecutor`: the number of cores to use (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_tg, rep = apply_to_fileset(data_manipulation=MyProcessor(),\n",
    "                            fileset=preprocessed_available,\n",
    "                            schemaclass=BaseSchema,\n",
    "                            uproot_options={\"allow_read_errors_with_report\": (OSError, KeyError)},\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out, rep = dask.compute(full_tg, rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this ran faster than the previous cell, but that may depend on how many cores are available on the machine you are running this notebook and your connection to `eospublic.cern.ch`. At least the output will be prettier now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "out[\"DoubleMuon\"][\"mass\"].plot1d(ax=ax)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend(title=\"Dimuon charge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting fancy\n",
    "Let's flesh out this analysis into a 4-muon analysis, searching for diboson events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numba\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def find_4lep(events_leptons, builder):\n",
    "    \"\"\"Search for valid 4-lepton combinations from an array of events * leptons {charge, ...}\n",
    "    \n",
    "    A valid candidate has two pairs of leptons that each have balanced charge\n",
    "    Outputs an array of events * candidates {indices 0..3} corresponding to all valid\n",
    "    permutations of all valid combinations of unique leptons in each event\n",
    "    (omitting permutations of the pairs)\n",
    "    \"\"\"\n",
    "    for leptons in events_leptons:\n",
    "        builder.begin_list()\n",
    "        nlep = len(leptons)\n",
    "        for i0 in range(nlep):\n",
    "            for i1 in range(i0 + 1, nlep):\n",
    "                if leptons[i0].charge + leptons[i1].charge != 0:\n",
    "                    continue\n",
    "                for i2 in range(nlep):\n",
    "                    for i3 in range(i2 + 1, nlep):\n",
    "                        if len({i0, i1, i2, i3}) < 4:\n",
    "                            continue\n",
    "                        if leptons[i2].charge + leptons[i3].charge != 0:\n",
    "                            continue\n",
    "                        builder.begin_tuple(4)\n",
    "                        builder.index(0).integer(i0)\n",
    "                        builder.index(1).integer(i1)\n",
    "                        builder.index(2).integer(i2)\n",
    "                        builder.index(3).integer(i3)\n",
    "                        builder.end_tuple()\n",
    "        builder.end_list()\n",
    "\n",
    "    return builder\n",
    "\n",
    "\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self, events):\n",
    "        dataset_axis = hist.axis.StrCategory([], growth=True, name=\"dataset\", label=\"Primary dataset\")\n",
    "        mass_axis = hist.axis.Regular(300, 0, 300, name=\"mass\", label=r\"$m_{\\mu\\mu}$ [GeV]\")\n",
    "        pt_axis = hist.axis.Regular(300, 0, 300, name=\"pt\", label=r\"$p_{T,\\mu}$ [GeV]\")\n",
    "        \n",
    "        h_nMuons = Hist(\n",
    "            dataset_axis,\n",
    "            hist.axis.IntCategory(range(6), name=\"nMuons\", label=\"Number of good muons\"),\n",
    "            storage=\"weight\", label=\"Counts\",\n",
    "        )\n",
    "        h_m4mu = Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_mZ1 = Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_mZ2 = Hist(dataset_axis, mass_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_ptZ1mu1 = Hist(dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\")\n",
    "        h_ptZ1mu2 = Hist(dataset_axis, pt_axis, storage=\"weight\", label=\"Counts\")\n",
    "                \n",
    "        #cutflow = defaultdict(int)\n",
    "        \n",
    "        dataset = events.metadata['dataset']\n",
    "        muons = ak.zip({\n",
    "            \"pt\": events.Muon_pt,\n",
    "            \"eta\": events.Muon_eta,\n",
    "            \"phi\": events.Muon_phi,\n",
    "            \"mass\": events.Muon_mass,\n",
    "            \"charge\": events.Muon_charge,\n",
    "            \"softId\": events.Muon_softId,\n",
    "            \"isolation\": events.Muon_pfRelIso03_all,\n",
    "        }, with_name=\"PtEtaPhiMCandidate\", behavior=candidate.behavior)\n",
    "        \n",
    "        # make sure they are sorted by transverse momentum\n",
    "        muons = muons[ak.argsort(muons.pt, axis=1)]\n",
    "        \n",
    "        #cutflow['all events'] += len(muons)\n",
    "        \n",
    "        # impose some quality and minimum pt cuts on the muons\n",
    "        muons = muons[\n",
    "            muons.softId\n",
    "            & (muons.pt > 5)\n",
    "            & (muons.isolation < 0.2)\n",
    "        ]\n",
    "        #cutflow['at least 4 good muons'] += ak.sum(ak.num(muons) >= 4)\n",
    "        h_nMuons.fill(dataset=dataset, nMuons=ak.num(muons))\n",
    "        \n",
    "        # reduce first axis: skip events without enough muons\n",
    "        muons = muons[ak.num(muons) >= 4]\n",
    "        \n",
    "        # find all candidates with helper function\n",
    "        fourmuon = find_4lep(muons, ak.ArrayBuilder()).snapshot()\n",
    "        if ak.all(ak.num(fourmuon) == 0):\n",
    "            # skip processing as it is an EmptyArray\n",
    "            return {\n",
    "                'nMuons': h_nMuons,\n",
    "                'cutflow': {dataset: cutflow},\n",
    "            }\n",
    "        fourmuon = [muons[fourmuon[idx]] for idx in \"0123\"]\n",
    "        fourmuon = ak.zip({\n",
    "            \"z1\": ak.zip({\n",
    "                \"lep1\": fourmuon[0],\n",
    "                \"lep2\": fourmuon[1],\n",
    "                \"p4\": fourmuon[0] + fourmuon[1],\n",
    "            }),\n",
    "            \"z2\": ak.zip({\n",
    "                \"lep1\": fourmuon[2],\n",
    "                \"lep2\": fourmuon[3],\n",
    "                \"p4\": fourmuon[2] + fourmuon[3],\n",
    "            }),\n",
    "        })\n",
    "        \n",
    "        #cutflow['at least one candidate'] += ak.sum(ak.num(fourmuon) > 0)\n",
    "         \n",
    "        # require minimum dimuon mass\n",
    "        fourmuon = fourmuon[(fourmuon.z1.p4.mass > 60.) & (fourmuon.z2.p4.mass > 20.)]\n",
    "        #cutflow['minimum dimuon mass'] += ak.sum(ak.num(fourmuon) > 0)\n",
    "        \n",
    "        # choose permutation with z1 mass closest to nominal Z boson mass\n",
    "        bestz1 = ak.singletons(ak.argmin(abs(fourmuon.z1.p4.mass - 91.1876), axis=1))\n",
    "        fourmuon = ak.flatten(fourmuon[bestz1])\n",
    "        \n",
    "        h_m4mu.fill(\n",
    "            dataset=dataset,\n",
    "            mass=(fourmuon.z1.p4 + fourmuon.z2.p4).mass,\n",
    "        )\n",
    "        h_mZ1.fill(\n",
    "            dataset=dataset, \n",
    "            mass=fourmuon.z1.p4.mass,\n",
    "        )\n",
    "        h_mZ2.fill(\n",
    "            dataset=dataset, \n",
    "            mass=fourmuon.z2.p4.mass,\n",
    "        )\n",
    "        h_ptZ1mu1.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep1.pt,\n",
    "        )\n",
    "        h_ptZ1mu2.fill(\n",
    "            dataset=dataset,\n",
    "            pt=fourmuon.z1.lep2.pt,\n",
    "        )\n",
    "        return {\n",
    "            'nMuons': h_nMuons,\n",
    "            'mass': h_m4mu,\n",
    "            'mass_z1': h_mZ1,\n",
    "            'mass_z2': h_mZ2,\n",
    "            'pt_z1_mu1': h_ptZ1mu1,\n",
    "            'pt_z1_mu2': h_ptZ1mu2,\n",
    "            #'cutflow': {dataset: cutflow},\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "\n",
    "fancy_tg, fancy_rep = apply_to_fileset(data_manipulation=FancyDimuonProcessor(),\n",
    "                                       fileset=preprocessed_available,\n",
    "                                       schemaclass=BaseSchema,\n",
    "                                       uproot_options={\"allow_read_errors_with_report\": (OSError, KeyError)},\n",
    "                                      )\n",
    "output, realized_rep = dask.compute(fancy_tg, fancy_rep)\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevt = output['cutflow']['ZZ to 4mu']['all events'] + output['cutflow']['DoubleMuon']['all events']\n",
    "print(\"Events/s:\", nevt / elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is just us looking at the output, you can execute it if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale ZZ simulation to expected yield\n",
    "lumi = 11.6  # 1/fb\n",
    "zzxs = 7200 * 0.0336**2  # approximate 8 TeV ZZ(4mu)\n",
    "nzz = output['cutflow']['ZZ to 4mu']['all events']\n",
    "\n",
    "scaled = {}\n",
    "for name, h in output.items():\n",
    "    if isinstance(h, hist.Hist):\n",
    "        scaled[name] = h.copy()\n",
    "        scaled[name].view()[0, :] *= lumi * zzxs / nzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scaled['nMuons'].plot1d(ax=ax, overlay='dataset')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['mass'][:, ::hist.rebin(4)].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['mass_z1'].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['mass_z2'].plot1d(ax=ax, overlay='dataset')\n",
    "ax.set_xlim(2, 300)\n",
    "# ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['pt_z1_mu1'].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "scaled['pt_z1_mu2'].plot1d(ax=ax, overlay='dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
